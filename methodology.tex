\section{Methodology} \label{sec:method}


%\emph{How can we concretely evaluate the accuracy and reliability of statistical estimators in approximating maximum reachability $S$ in fuzzing campaigns?}

Due to the limitations discussed in \autoref{sec:reachability} regarding current approaches for obtaining ground truth in maximum reachability estimators, we propose a novel methodology that enables the derivation of the exact ground truth for a fuzzing subject. This approach leverages the syntactic constraints of input formats via context-free grammars and guides input generation using control-flow feedback from the target program.

\subsection{Evaluating Estimators of \texorpdfstring{$\hat{S}$}{S-hat} Against Accurate Ground Truth}

Our method combines grammar-based input generation with dynamic program instrumentation to systematically explore the reachable code space of a program under test $\mathcal{P}$. The approach comprises the following key components: (i) defining a formal grammar to ensure syntactic validity of inputs, (ii) instrumenting the program to monitor execution paths, and (iii) using coverage feedback to adaptively generate new inputs that traverse previously unexplored control-flow paths.
\vspace{0.5em}

\noindent\textbf{Grammar-Based Input Generation: }We begin by defining a context-free grammar (CFG) that describes the structure of valid inputs accepted by the $\mathcal{P}$. This grammar can be constructed manually from the program specification, or extracted automatically using tools such as ANTLR or Tree-Sitter. The CFG enables the generation of syntactically valid test cases, reducing the likelihood of inputs being rejected early due to parsing errors.

Given a CFG $\mathcal{G}$, we construct a probabilistic grammar-based fuzzer that generates inputs by recursively expanding non-terminals according to production rules. To increase diversity, rule selection is initially uniform but is later biased using feedback from program execution.
\vspace{0.5em}

\noindent\textbf{Control-Flow Feedback and Coverage Tracking:} To guide the input generation process toward unexplored behavior, we instrument the $\mathcal{P}$ using a coverage measurement framework such as \texttt{gcov}, \texttt{LLVM Sanitizers}, or \texttt{AFL++}’s built-in instrumentation. We track code coverage metrics such as basic blocks, edge transitions, and function calls. For each generated input, the corresponding execution trace is analyzed to determine whether it results in new coverage.

Inputs that trigger novel control-flow paths are retained and used as seeds for further mutation and grammar-driven recombination. This ensures that exploration prioritizes semantically distinct regions of the program's execution space.
\vspace{0.5em}

\noindent\textbf{Adaptive Input Evolution: } To maximize coverage, we employ an adaptive fuzzing loop that continuously refines the input generation process based on historical coverage data. When an input results in increased coverage, the associated grammar rule probabilities are adjusted to favor similar structures in future generations. This mechanism biases the input generation process toward syntactic patterns that are more likely to exercise deeper or less frequently executed code paths.
\vspace{0.5em}

\noindent\textbf{Ground Truth Construction:} We terminate the fuzzing campaign once coverage plateaus, indicating that no further inputs lead to new control-flow discoveries over a predefined number of iterations. The union of all observed coverage paths is then used to construct an empirical upper bound on reachable program behavior. This set serves as the exact ground truth for evaluating maximum reachability estimators.
\vspace{0.5em}

\noindent\textbf{Scalability and Applicability:} This methodology is designed to be applicable to large-scale C programs that consist of thousands of lines of code and complex control-flow. By ensuring syntactic correctness and guiding input generation using precise control-flow instrumentation, our approach enables deep semantic exploration of the PUT without exhaustive enumeration or symbolic execution. Furthermore, it is compatible with modern fuzzing frameworks such as \texttt{AFL++}, \texttt{Superion}, and \texttt{Angora}, making it a practical foundation for evaluating reachability estimation techniques.

\subsection{Assessing Reliability of the Estimators of \texorpdfstring{$\hat{S}$}{S-hat}}

Prior empirical research on estimating maximum reachable coverage in fuzzing campaigns—under the Bernoulli product model—has shown that statistical estimators systematically underestimate true reachability until near saturation, a phenomenon termed \emph{false peaks} by \cite{reachability_2023}. This negative bias is largely inevitable in early stages due to the insufficiency of fuzzing data and the heterogeneous species diversity, which limits accurate approximation of the species distribution. These limitations prompt a critical question: \emph{How can we ensure that statistical estimators provide reliable approximations of maximum reachability $S$ in fuzzing campaigns?}

\result{\textbf{Intuition:} While incidence-based statistical estimators of fuzzing effectiveness are sensitive to the sampling unit size ($r$), a reliable estimator should produce overlapping point estimates for $S$—with varying variance but consistent expectation—when evaluated over equal-length campaigns.}

The incidence frequency counts—particularly singletons ($f_1$) and doubletons ($f_2$)—are sensitive to the granularity of the sampling unit ($r$). Using individual inputs ($r=1$) yields the most fine-grained data, while aggregating inputs (e.g., over time intervals) reduces resolution. Aggregation tends to decrease $f_1$ and increase higher-frequency counts, as rare elements may co-occur within a unit. Since estimators like Chao2 depend directly on $f_1$ and $f_2$, the point estimate of species richness $\hat{S}$ may vary with changes in $r$, even under equal-length campaigns. Despite this sensitivity, the choice of $r$ is often arbitrary; for instance, \cite{reachability_2023} adopts 15-minute intervals without justification.

However, since the maximum reachability $S$ is a finite quantity for any given fuzzing campaign, a reliable estimator should approximate it within reasonable error bounds—despite variability in incidence frequencies introduced by changes in the sampling unit definition—provided that the estimation is performed at the same time point since the campaign's start. Leveraging this property, we evaluate whether estimators of maximum reachability $S$ yield consistent point estimates with overlapping confidence intervals, while allowing for differences in estimation accuracy (i.e., variability) across fuzzing campaigns with different sampling unit definitions $r$.

\textcolor{red}{Stopping Criterion Evaluation:
Evaluate the stopping criterion in checking the reliability of reachability estimators. (Applicable beyond f1 <f2) By evaluating f1 actually becomes an upper bound to f0.
}
