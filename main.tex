% https://conf.researchr.org/track/icsme-2025/icsme-2025-registered-reports
% Submissions to the ICSME 2025 RR track must not exceed 6 pages (plus 1 additional page of references). The page limit is strict. All submissions must be in PDF and must be submitted online by the deadline via the ICSME 2025 EasyChair link.


% 20th May last date.
\documentclass[conference]{IEEEtran}

\IEEEoverridecommandlockouts


%\usepackage{csquotes}
\usepackage{cite}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{algorithm,algorithmic}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{multirow}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{hyperref}
\newcommand{\mytitle}{Assessing Reliability of Statistical Maximum Coverage Estimators in Fuzzing}
\hypersetup{pdftitle={\mytitle},
colorlinks=true,linkcolor=blue,citecolor=blue,filecolor=blue,urlcolor=blue
}
\usepackage{cleveref}

\usepackage{subfig,paralist}
\usepackage{pifont}
\usepackage{enumitem}
\usepackage{url}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow,array}
\usepackage{threeparttable}
%\usepackage{balance}
%\usepackage[group-separator={,}]{siunitx}
%\usepackage[capitalise]{cleveref}
\usepackage{bm}


\newcounter{todocounter}
\newcommand{\todo}[1]{\textcolor{red}{\stepcounter{todocounter}\textbf{TODO [\thetodocounter]:} \textit{#1}}}
% \newcommand{\done}[1]{\textcolor{green}{\stepcounter{todocounter}\textbf{DONE [\thetodocounter]:} \textit{#1}}}
\newcommand{\done}[1]{} % Hide DONE messages as per original

\newcommand{\recheck}[1]{\textcolor{red}{#1}} % Keep for draft
\newcommand{\revise}[1]{\textcolor{black}{#1}} % Keep for draft

% Hide TODOs for final version if needed
% \IfFileExists{SUBMIT}{ % Requires creating a file named SUBMIT
% \renewcommand{\todo}[1]{}
% \renewcommand{\done}[1]{}
% }{}
% \usepackage{colortbl} % Already loaded




\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}
\urlstyle{tt}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem{lemma}{Lemma}
%\renewcommand\qedsymbol{$\blacksquare$}
\newcommand*\diff{\mathop{}\!\mathrm{d}}

\usepackage{tcolorbox}% http://ctan.org/pkg/tcolorbox
\definecolor{mycolor}{rgb}{0.122, 0.435, 0.698}% Rule colour
\definecolor{gray1}{gray}{0.3}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    %backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\tiny\ttfamily,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    columns=fixed
}
\lstset{style=mystyle}

\newcommand{\result}[1]{%
\begin{tcolorbox}[colframe=black,boxrule=0.5pt,arc=4pt,
      left=6pt,right=6pt,top=6pt,bottom=6pt,boxsep=0pt,width=\columnwidth]%
      {\emph{#1}}
\end{tcolorbox}%
}

\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0}
\definecolor{darkred}{rgb}{0.82, 0.1, 0.26}
\newcommand{\cmark}{\textcolor{darkgreen}{\ding{51}}}%
\newcommand{\xmark}{\textcolor{darkred}{\ding{55}}\ }%

%\newcommand{\todo}[1]{\noindent\textcolor{red}{TODO:\xspace #1}}
%\newcommand{\chk}[1]{\noindent\textcolor{red}{#1\xspace}}

% Section labels
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\subsubsectionautorefname{Section}%%

%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.

\begin{document}

\title{\mytitle}

\author{
\IEEEauthorblockN{Danushka Liyanage}
\IEEEauthorblockA{\textit{School of Computer Science} \\
\textit{University of Sydney}\\
Sydney, Australia \\
danushka.liyanage@sydney.edu.au}
\and 
\IEEEauthorblockN{Rahul Gopinath}
\IEEEauthorblockA{\textit{School of Computer Science} \\
\textit{University of Sydney}\\
Sydney, Australia \\
rahul.gopinath@sydney.edu.au}
}

\maketitle

\thispagestyle{plain}
\pagestyle{plain} 


\begin{abstract}
Fuzzers are often guided by coverage, making the estimation of maximum
possible coverage a key concern in fuzzing.
However, achieving 100\% coverage is impossible for most real-world
software systems, regardless of the time spent fuzzing.
While static analysis of reachability can provide an upper bound,
it is often highly inaccurate.
Recently, statistical estimation based on species richness estimators
in biostatistics have been proposed as a potential solution.
However, a lack of reliable benchmarks with labeled ground truth has
limited a thorough evaluation of the accuracy of this approach.

To address this challenge, we propose an evaluation framework
that generates complex programs with real-world control flow synthetically
where 100\% reachability is assured, providing the
ground-truth for evaluation.
Secondly, we propose a novel check for the reliability of estimators---by
varying the size of sampling units which, under theory should not impact
the estimation.

Using these benchmarks, we propose to empirically evaluate various
species richness estimators under different sampling units.
We aim to identify which estimators demonstrate the highest accuracy against
known ground truth and consistency across sampling units.
Our work will identify the most reliable approaches to estimating maximum
possible coverage in real-world fuzzing campaigns.
\end{abstract}


\section{Introduction}
Fuzzing is an automated testing technique that leverages guidance provided
by coverage. The budget allocated to fuzzing, and hence, the stopping criteria
of fuzzing is strongly determined by the maximum possible coverage that can
be obtained~\cite{fell2017review}.

However, achieving 100\% coverage in software testing is an impossible goal for
complex real-world programs~\cite{horgan1994achieving}.
This is due to the increase in input domain and the corresponding
increase in execution spaces, which makes exhaustive testing
impossible~\cite{knight1996exhaustive}.

Yet, quantifying the maximum possible coverage (also called \emph{maximum reachability})
remains a challenge, which raises a fundamental question:
\emph{How close is a fuzzing campaign to achieving its maximum reachable coverage?}

Recent researches have proposed \emph{species richness estimators} from
biostatistics~\cite{chao2016species} as a principled approach to estimate
the reachable coverage of a fuzzing campaign~\cite{stads}.
These estimators treat fuzzing as a statistical sampling process in which
each test input belongs to one or more program execution behaviors,
for example, reaching a program element, which can then be leveraged to
estimate the total possible number of such behaviours such as the total
possible reachable elements in a program~\cite{stads}.
% (e.g., a branch, path, function, or bug).

% Under this analogy, statistical estimators such as Chao2 \cite{chao2017thirty} have been explored to approximate total reachable coverage from partially observed data.

However, evaluating the accuracy and reliability
%(in other words bias and variability)
of these estimators is difficult. The main issue is that, to evaluate such
estimators, we need a benchmark of several large complex programs where
maximum reachability is known. Unfortunately, establishing the maximum
reachability is impossible except for trivial programs. Hence,
rearchers have resorted to using small
programs, fuzzed to saturation as an alternative~\cite{reachability_2023}.
% Using this approach, they have further shown that the reliability of such
% estimators increase when the number of singletons (those elements that
% are reached by a single)
This approach is not ideal as performance of an estimator in small
programs may not representative of the performance of an estimator in complex
real-world programs.

We propose an alternative approach to overcome this difficulty. We note that
in programs such as parsers (which are one of the major subjects for fuzzing)
the control-flow of a program along with its data-flow determines the
complexity of the program, and hence, determines the difficulty
experienced in reaching the program elements. we also note that the
control-flow of any structured program can be represented as a context-free
grammar~\Cref{fig:cfg}.

The interesting fact here is that it is possible to generate arbitrary
context-free grammars. Furthermore, one can also fine-tune the complexity
of such grammars generated based on several dimensions such as the size
of the grammar, the number of direct and indirect recursions allowed, the
number of linear-recursive rules etc.
% , the general size of the language
% corresponding to the universe of all strings, and finally the VC-dimension
% of the language.
We further note that converting a context-free grammar to recursive descent
parsers is well known, and produces parsers which have similarly complex
control-flows.

Our proposal is then to leverage such arbitrary generated complex
context-free grammars into parsers, and use such parsers as benchmarks for
estimating the reliability of reachability estimators. We propose to compare
the difficulty of fuzzers when faced with real-world parsers to these
generated parsers to ensure that the generated benchmarks are similar to
real-world programs in complexity. If they prove equivalent in complexity,
such generated, and ever-renewable benchmarks may not only be suitable for
evaluating the reliability of estimators, but also for other evaluation
tasks that require complex programs such as evaluating fuzzers,
fault localization techniques, program repair techniques, and so on.

\begin{figure*}
  \includegraphics[width=\textwidth]{basiccfg.pdf}
  \caption{Control flow to context-free grammar}
  \label{fig:cfg}
\end{figure*}

While synthetic benchmarks are a useful alternative, is it possible to use
real-world programs even if ground-truth (the maximum reachability) is not
known? A recent research on evaluating the reliability of biostatistics based
estimators for mutation analysis~\cite{kuznetsov2024empirical} provides a
solution. The researchers point out that the \emph{sampling units} used in
species richness estimators should not have a significant effect on the final
estimate. In previous research on coverage estimation~\cite{reachability_2023},
the sampling-unit used is the time interval. That is, the incidence data of
each 15-minutes bin was used as the sampling unit. Since the 15-minute interval
is arbitrary, it stands to reason that changing the timing granularity should
not impact the final estimate. Hence, we propose to evaluate the reliability of
maximum coverage estimators under varying granularity of sampling interval,
for the same fuzzing campaign duration.

% To be considered reliable, an estimator should yield consistent point estimates of maximum reachability across different sampling unit definitions, provided the total campaign length is held constant.

% Hence, in this paper, we propose to systematically investigate the reliability
% of incidence-based statistical estimators for maximum coverage.

\noindent{}\textbf{Contributions.}

\begin{itemize}
  \item \textbf{Synthetic benchmarks for evaluation:} We propose a novel
framework to generate synthetic program benchmarks that emulates real-world
programs, and use this to evaluate maximum reachability estimators.

  \item \textbf{Sampling-unit based evaluation of reliability} We will evaluate
    the reliability of maximum reachability estimators on 32 real-world programs
    by comparing the estimates under sampling units of diverse granularity.

  % We evaluate this framework on 32 real-world programs and demonstrate that certain estimators yield overlapping confidence intervals and reduced variance across different granularities—indicating estimator reliability.
    
% \item \textbf{Assessment of Stopping Criteria:} We re-examine previously proposed stopping criteria and evaluate their effectiveness in identifying when a statistical estimate may serve as a reliable upper bound on maximum reachability.

%\item\textcolor{red}{In this paper we propose a solution to the question of how to interpret the estimate from Chao on time based samples.}
\end{itemize}

The remainder of this paper is organized as follows. Section~\ref{sec:model} introduces the probabilistic model underlying fuzzing and the assumptions made in statistical reachability estimation. Section~\ref{sec:reachability} discusses current approaches to reachability estimation in fuzzing, including existing methods, their associated challenges, proposed solutions, and known drawbacks. Section~\ref{sec:method} presents our methodology for assessing the reliability of statistical estimators of maximum reachability. Section~\ref{sec:setup} details the experimental setup used to evaluate our approach. Finally, Section~\ref{sec:conclusion} concludes the paper and outlines directions for future research.

% ---- Model
\section{Preliminaries} \label{sec:model}

\subsection{Probabilistic Model for Fuzzing}

The STADS framework~\cite{stads,residual2021,bedivfuzz,entropic} formalizes
fuzzing as a statistical sampling process,
%We adopt this statistical perspective from STADS \cite{stads,residual2021,bedivfuzz,entropic}, 
where fuzzing is modeled as a sampling process $\mathcal{F}$,
in which test inputs are drawn with replacement from the
program input space $\pmb{\mathcal{D}}$.
Formally, $\mathcal{F}$ is defined as a sequence of $N$ independent and identically distributed (i.i.d.) random variables:

\begin{equation*}
    \mathcal{F}=\{X_n \mid X_n \in \pmb{\mathcal{D}}\}_{n=1}^N
\end{equation*}

We partition the input space $\pmb{\mathcal{D}}$ into $S$ overlapping subdomains
$\{\mathcal{D}_i\}_{i=1}^S$, where each subdomain corresponds to a \emph{coverage element}. An input $X_n \in \mathcal{F}$ is said to cover a \emph{new} coverage element $\mathcal{D}_i$ if $X_n \in \mathcal{D}_i$ and no previously sampled input $X_m \in \mathcal{F}$ (for $m < n$) belongs to $\mathcal{D}_i$, meaning $\mathcal{D}_i$ is sampled for the first time.

\subsection{Bernoulli Product Model}

Since each input in fuzzing can cover \emph{one or more coverage elements}, we record coverage information as \emph{sampling unit-based incidence data} \cite{colwell2012models,Chao_etal:2017} and model it using the Bernoulli Product model. This approach is more practical, as it eliminates the need to record data for every individual test input in a fuzzing process that generates thousands of inputs per second. Instead, multiple inputs can be grouped into a \emph{sampling unit}.

For each sampling unit, the fuzzing data indicate whether a coverage element has been reached. Let $\pi_i$ denote the probability that a sampling unit covers element $\mathcal{D}_i$, assuming $\pi_i$ remains constant across all randomly selected sampling units. In general, the sum of all $\pi_i$ values does \emph{not} equal unity.

During a fuzzing campaign, suppose we record $t$ sampling units. The incidence data form an element-by-sampling-unit incidence matrix ${W_{ij};i=1,2,\dots,S,j=1,2,\dots,t}$ with $S$ rows and $t$ columns, where $W_{ij} = 1$ if element $i$ is covered in sampling unit $j$, and $W_{ij} = 0$ otherwise.

The incidence frequency $Y_i$ represents the number of sampling units in which element $\mathcal{D}_i$ is covered; i.e., $Y_i=\sum_{j=1}^{t}W_{ij}$. A coverage element $\mathcal{D}_i$ that has not been covered by any sampling unit will have an incidence frequency of zero; i.e., $Y_i=0$.

Given the set of detection probabilities $(\pi_1,\pi_2,\dots,\pi_S)$, we assume each element $W_{ij}$ in the incidence matrix follows a Bernoulli distribution with probability $\pi_i$. The probability distribution for the incidence matrix is:

\begin{equation}
    \begin{split}
        P(W_{ij}=w_{ij};i=1,2,\dots,S,j=1,2,\dots,t) \\
        = \prod_{j=1}^{t}\prod_{i=1}^{S}\pi_i^{w_{ij}}(1-\pi_i)^{1-w_{ij}} \\
        = \prod_{i=1}^{S}\pi_i^{y_i}(1-\pi_i)^{t-y_i}.
    \end{split}
\end{equation}

The marginal distribution for the incidence-based frequency $Y_i$ for the $i$-th coverage element follows a binomial distribution characterized by $t$ and the detection probability $\pi_i$:

\begin{equation}
    P(Y_i=y_i) = \binom{t}{y_i}\pi_i^{y_i}(1-\pi_i)^{t-y_i}, \qquad i=1,2,\dots,S.
\end{equation}

Denote the incidence frequency counts by $(f_0, f_1, \dots, f_t)$, where $f_k$ is the number of elements covered in exactly $k$ sampling units in the data, $k=0,1,\dots,t$. Here, $f_1$ represents the number of ``singleton" elements (those that are covered in only one sampling unit), and $f_2$ represents the number of ``doubleton" elements (those that are covered in exactly two sampling units). The unobservable zero frequency count $f_0$ denotes the number of coverage elements that are not covered by any of the $t$ sampling units. Then, the number of covered elements in the current campaign is $S(t)=\sum_{i>0}f_i$, and $S(t)+f_0=S$.


%During a fuzzing campaign where we generate enormous amount of test inputs per unit time (at least thousands of inputs within seconds), it is not quite not practical to save the outcome for each input (i.e. the species $\mathcal{D}_i$ that our $X_n$ belongs to) in our data storage. Yet, it is a common practice to write such data at regular intervals. For instance, in our experimentation of 7-day fuzzing campaigns, we record incidence data once in every 15 minutes. Statistically, we can consider all the inputs generated between two consecutive recording instances as a sample of inputs and we indicate the detection/non-detection of species in this sample as a record. 
%

%Below we explain the probabilistic model for our fuzzing data. 
%
%\subsection{Probabilistic Model for Sampling-Unit-Based Data} \label{sec:probmodel}
%
%Assuming that the fuzzer's search space contains $S$ species, indexed by ${1,2,\dots,S}$. Without loss of generality, assume we have sampled $t$ sampling units, indexed by $1,2,\dots,t$, where each sampling unit represents a set of test inputs generated during a certain period of the ongoing fuzzing campaign. A species could be any choice of a program behaviour such as bugs or coverage elements like program paths, branches, or functions. It is assumed that these sampling units are randomly and independently sampled.
% ---- End Model
\input{estimation.tex}
\input{methodology.tex}
\input{setup.tex}
\input{related.tex}


\section{Conclusion}
\label{sec:conclusion}
In this report, we propose to assess the reliability of species estimators when
they are used for estimating reachable coverage. We propose to do that both by
providing a synthetic benchmark with labelled ground truth, and also by
using a separate means of checking the reliability of estimators by variying
the sampling units used.

%\section{Authors and Affiliations}


\bibliographystyle{IEEEtran}
\bibliography{references.bib}


%\section{Acknowledgments}

%% If your work has an appendix, this is the place to put it.
%\appendix

\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
