% https://conf.researchr.org/track/icsme-2025/icsme-2025-registered-reports
% Submissions to the ICSME 2025 RR track must not exceed 6 pages (plus 1 additional page of references). The page limit is strict. All submissions must be in PDF and must be submitted online by the deadline via the ICSME 2025 EasyChair link.


% 20th May last date.
\documentclass[conference]{IEEEtran}

\IEEEoverridecommandlockouts


%\usepackage{csquotes}
\usepackage{cite}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{algorithm,algorithmic}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{multirow}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{hyperref}
\newcommand{\mytitle}{Assessing Reliability of Statistical Maximum Coverage Estimators in Fuzzing}
\hypersetup{pdftitle={\mytitle},
colorlinks=true,linkcolor=blue,citecolor=blue,filecolor=blue,urlcolor=blue
}
\usepackage{cleveref}

\usepackage{subfig,paralist}
\usepackage{pifont}
\usepackage{enumitem}
\usepackage{url}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow,array}
\usepackage{threeparttable}
%\usepackage{balance}
%\usepackage[group-separator={,}]{siunitx}
%\usepackage[capitalise]{cleveref}
\usepackage{bm}


\newcounter{todocounter}
\newcommand{\todo}[1]{\marginpar{$|$}\textcolor{red}{\stepcounter{todocounter}\footnote[\thetodocounter]{\textcolor{red}{\textbf{TODO }}\textit{#1}}}}
\newcommand{\done}[1]{\marginpar{$*$}\textcolor{green}{\stepcounter{todocounter}\footnote[\thetodocounter]{\textcolor{black}{\textbf{DONE }}\textit{#1}}}}

\newcommand{\recheck}[1]{\textcolor{red}{#1}}
\newcommand{\revise}[1]{\textcolor{black}{#1}}

\renewcommand{\done}[1]{} % comment to see responses.

% Hide TODOs for final version if needed
% \IfFileExists{SUBMIT}{ % Requires creating a file named SUBMIT
% \renewcommand{\todo}[1]{}
% \renewcommand{\done}[1]{}
% }{}
% \usepackage{colortbl} % Already loaded




\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}
\urlstyle{tt}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem{lemma}{Lemma}
%\renewcommand\qedsymbol{$\blacksquare$}
\newcommand*\diff{\mathop{}\!\mathrm{d}}

\usepackage{tcolorbox}% http://ctan.org/pkg/tcolorbox
\definecolor{mycolor}{rgb}{0.122, 0.435, 0.698}% Rule colour
\definecolor{gray1}{gray}{0.3}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    %backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\tiny\ttfamily,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    columns=fixed
}
\lstset{style=mystyle}

\newcommand{\result}[1]{%
\begin{tcolorbox}[colframe=black,boxrule=0.5pt,arc=4pt,
      left=6pt,right=6pt,top=6pt,bottom=6pt,boxsep=0pt,width=\columnwidth]%
      {\emph{#1}}
\end{tcolorbox}%
}

\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0}
\definecolor{darkred}{rgb}{0.82, 0.1, 0.26}
\newcommand{\cmark}{\textcolor{darkgreen}{\ding{51}}}%
\newcommand{\xmark}{\textcolor{darkred}{\ding{55}}\ }%

%\newcommand{\todo}[1]{\noindent\textcolor{red}{TODO:\xspace #1}}
%\newcommand{\chk}[1]{\noindent\textcolor{red}{#1\xspace}}

% Section labels
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\subsubsectionautorefname{Section}%%

%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.

\begin{document}

\title{\mytitle}

\author{
\IEEEauthorblockN{Danushka Liyanage}
\IEEEauthorblockA{\textit{School of Computer Science} \\
\textit{University of Sydney}\\
Sydney, Australia \\
danushka.liyanage@sydney.edu.au}
\and 
\IEEEauthorblockN{Rahul Gopinath}
\IEEEauthorblockA{\textit{School of Computer Science} \\
\textit{University of Sydney}\\
Sydney, Australia \\
rahul.gopinath@sydney.edu.au}
}

\maketitle

\thispagestyle{plain}
\pagestyle{plain} 


\begin{abstract}
Fuzzers are often guided by coverage, making the estimation of maximum
possible coverage a key concern in fuzzing.
However, achieving 100\% coverage is impossible for most real-world
software systems, regardless of the time spent fuzzing.
While static analysis of reachability can provide an upper bound,
it is often highly inaccurate.
Recently, statistical estimation based on species richness estimators
in biostatistics have been proposed as a potential solution.
However, a lack of reliable benchmarks with labeled ground truth has
limited a thorough evaluation of the accuracy of this approach.

To address this challenge, we propose an evaluation framework
that generates complex programs with real-world control flow synthetically
where 100\% reachability is assured, providing the
ground-truth for evaluation.
Secondly, we propose a novel check for the reliability of estimators---by
varying the size of sampling units which, under theory should not impact
the estimation.

Using these benchmarks, we propose to empirically evaluate various
species richness estimators under different sampling units.
We aim to identify which estimators demonstrate the highest accuracy against
known ground truth and consistency across sampling units.
Our work will identify the most reliable approaches to estimating maximum
possible coverage in real-world fuzzing campaigns.
\end{abstract}


\section{Introduction}
Fuzzing is an automated testing technique that leverages guidance provided
by coverage. The budget allocated to fuzzing, and hence, the stopping criteria
of fuzzing is strongly determined by the maximum possible coverage that can
be obtained~\cite{fell2017review}.

However, achieving 100\% coverage in software testing is an impossible goal for
complex real-world programs~\cite{horgan1994achieving}.
This is due to the increase in input domain and the corresponding
increase in execution spaces, which makes exhaustive testing
impossible~\cite{knight1996exhaustive}.

Yet, quantifying the maximum possible coverage (also called \emph{maximum reachability})
remains a challenge, which raises a fundamental question:
\emph{How close is a fuzzing campaign to achieving its maximum reachable coverage?}

Recent researches have proposed \emph{species richness estimators} from
biostatistics~\cite{chao2016species} as a principled approach to estimate
the reachable coverage of a fuzzing campaign~\cite{stads}.
These estimators treat fuzzing as a statistical sampling process in which
each test input belongs to one or more program execution behaviors,
for example, reaching a program element, which can then be leveraged to
estimate the total possible number of such behaviours such as the total
possible reachable elements in a program~\cite{stads}.
% (e.g., a branch, path, function, or bug).

% Under this analogy, statistical estimators such as Chao2 \cite{chao2017thirty} have been explored to approximate total reachable coverage from partially observed data.

However, evaluating the accuracy and reliability
%(in other words bias and variability)
of these estimators is difficult. The main issue is that, to evaluate such
estimators, we need a benchmark of several large complex programs where
maximum reachability is known. Unfortunately, establishing the maximum
reachability is impossible except for trivial programs. Hence,
rearchers have resorted to using small
programs, fuzzed to saturation as an alternative~\cite{reachability_2023}.
% Using this approach, they have further shown that the reliability of such
% estimators increase when the number of singletons (those elements that
% are reached by a single)
This approach is not ideal as performance of an estimator in small
programs may not representative of the performance of an estimator in complex
real-world programs.

We propose an alternative approach to overcome this difficulty. We note that
in programs such as parsers (which are one of the major subjects for fuzzing)
the control-flow of a program along with its data-flow determines the
complexity of the program, and hence, determines the difficulty
experienced in reaching the program elements. we also note that the
control-flow of any structured program can be represented as a context-free
grammar~\Cref{fig:cfg}.

The interesting fact here is that it is possible to generate arbitrary
context-free grammars. Furthermore, one can also fine-tune the complexity
of such grammars generated based on several dimensions such as the size
of the grammar, the number of direct and indirect recursions allowed, the
number of linear-recursive rules etc.
% , the general size of the language
% corresponding to the universe of all strings, and finally the VC-dimension
% of the language.
We further note that converting a context-free grammar to recursive descent
parsers is well known, and produces parsers which have similarly complex
control-flows.

Our proposal is then to leverage such arbitrary generated complex
context-free grammars into parsers, and use such parsers as benchmarks for
estimating the reliability of reachability estimators. We propose to compare
the difficulty of fuzzers when faced with real-world parsers to these
generated parsers to ensure that the generated benchmarks are similar to
real-world programs in complexity. If they prove equivalent in complexity,
such generated, and ever-renewable benchmarks may not only be suitable for
evaluating the reliability of estimators, but also for other evaluation
tasks that require complex programs such as evaluating fuzzers,
fault localization techniques, program repair techniques, and so on.

\begin{figure*}
  \includegraphics[width=\textwidth]{basiccfg.pdf}
  \caption{Control flow to context-free grammar}
  \label{fig:cfg}
\end{figure*}

While synthetic benchmarks are a useful alternative, is it possible to use
real-world programs even if ground-truth (the maximum reachability) is not
known? A recent research on evaluating the reliability of biostatistics based
estimators for mutation analysis~\cite{kuznetsov2024empirical} provides a
solution. The researchers point out that the \emph{sampling units} used in
species richness estimators should not have a significant effect on the final
estimate. In previous research on coverage estimation~\cite{reachability_2023},
the sampling-unit used is the time interval. That is, the incidence data of
each 15-minutes bin was used as the sampling unit. Since the 15-minute interval
is arbitrary, it stands to reason that changing the timing granularity should
not impact the final estimate. Hence, we propose to evaluate the reliability of
maximum coverage estimators under varying granularity of sampling interval,
for the same fuzzing campaign duration.

% To be considered reliable, an estimator should yield consistent point estimates of maximum reachability across different sampling unit definitions, provided the total campaign length is held constant.

% Hence, in this paper, we propose to systematically investigate the reliability
% of incidence-based statistical estimators for maximum coverage.

\noindent{}\textbf{Contributions.}

\begin{itemize}
  \item \textbf{Synthetic benchmarks for evaluation:} We propose a novel
framework to generate synthetic program benchmarks that emulates real-world
programs, and use this to evaluate maximum reachability estimators.

  \item \textbf{Sampling-unit based evaluation of reliability} We will evaluate
    the reliability of maximum reachability estimators on 32 real-world programs
    by comparing the estimates under sampling units of diverse granularity.

  % We evaluate this framework on 32 real-world programs and demonstrate that certain estimators yield overlapping confidence intervals and reduced variance across different granularities—indicating estimator reliability.
    
% \item \textbf{Assessment of Stopping Criteria:} We re-examine previously proposed stopping criteria and evaluate their effectiveness in identifying when a statistical estimate may serve as a reliable upper bound on maximum reachability.

%\item\textcolor{red}{In this paper we propose a solution to the question of how to interpret the estimate from Chao on time based samples.}
\end{itemize}

The remainder of this paper is organized as follows. Section~\ref{sec:model} introduces the probabilistic model underlying fuzzing and the assumptions made in statistical reachability estimation. Section~\ref{sec:reachability} discusses current approaches to reachability estimation in fuzzing, including existing methods, their associated challenges, proposed solutions, and known drawbacks. Section~\ref{sec:method} presents our methodology for assessing the reliability of statistical estimators of maximum reachability. Section~\ref{sec:setup} details the experimental setup used to evaluate our approach. Finally, Section~\ref{sec:conclusion} concludes the paper and outlines directions for future research.

% ---- Model
\section{Preliminaries} \label{sec:model}

\subsection{Probabilistic Model for Fuzzing}

The STADS framework~\cite{stads,residual2021,bedivfuzz,entropic} formalizes
fuzzing as a statistical sampling process,
%We adopt this statistical perspective from STADS \cite{stads,residual2021,bedivfuzz,entropic}, 
where fuzzing is modeled as a sampling process $\mathcal{F}$,
in which test inputs are drawn with replacement from the
program input space $\pmb{\mathcal{D}}$.
Formally, $\mathcal{F}$ is defined as a sequence of $N$ independent and identically distributed (i.i.d.) random variables:

\begin{equation*}
    \mathcal{F}=\{X_n \mid X_n \in \pmb{\mathcal{D}}\}_{n=1}^N
\end{equation*}

The STADS framework partitions the input space $\pmb{\mathcal{D}}$ into $S$ overlapping subdomains
$\{\mathcal{D}_i\}_{i=1}^S$, where each subdomain corresponds to a \emph{coverage element}. An input $X_n \in \mathcal{F}$ is said to cover a \emph{new} coverage element $\mathcal{D}_i$ if $X_n \in \mathcal{D}_i$ and no previously sampled input $X_m \in \mathcal{F}$ (for $m < n$) belongs to $\mathcal{D}_i$, meaning $\mathcal{D}_i$ is sampled for the first time.

\subsection{Bernoulli Product Model}
Since each input in fuzzing can cover \emph{one or more coverage elements},
STADS records coverage information as \emph{sampling unit-based incidence data}~\cite{colwell2012models,Chao_etal:2017}, where a sampling-unit is an aggregation of inputs during a fixed interval of time.
This is then modeled using the Bernoulli Product model.
This approach of grouping data into sampling-units is necessary to avoid having
to record detailed records for each input during fuzzing.
%This approach eliminates the need to record data for every individual test input in a fuzzing process that generates thousands of inputs per second. Instead, multiple inputs can be grouped into a \emph{sampling unit}.

For each sampling unit, the collected data indicates whether a coverage element
has been reached. Let $\pi_i$ denote the probability that a sampling unit covers
element $\mathcal{D}_i$, assuming $\pi_i$ remains constant across all randomly
selected sampling units.
In general, the sum of all $\pi_i$ values does \emph{not} equal unity.

During a fuzzing campaign, suppose we record $t$ sampling units. The incidence data forms an element$\times$sampling-unit incidence matrix ${W_{ij};i=1,2,\dots,S,j=1,2,\dots,t}$ with $S$ rows and $t$ columns, where $W_{ij} = 1$ if element $i$ is covered in sampling unit $j$, and $W_{ij} = 0$ otherwise.

The incidence frequency $Y_i$ represents the number of sampling units in which element $\mathcal{D}_i$ is covered; i.e., $Y_i=\sum_{j=1}^{t}W_{ij}$. A coverage element $\mathcal{D}_i$ that has not been covered by any sampling unit will have an incidence frequency of zero; i.e., $Y_i=0$.

Given the set of detection probabilities $(\pi_1,\pi_2,\dots,\pi_S)$, we assume each element $W_{ij}$ in the incidence matrix follows a Bernoulli distribution with probability $\pi_i$. The probability distribution for the incidence matrix is:

\begin{equation}
    \begin{split}
        P(W_{ij}=w_{ij};i=1,2,\dots,S,j=1,2,\dots,t) \\
        = \prod_{j=1}^{t}\prod_{i=1}^{S}\pi_i^{w_{ij}}(1-\pi_i)^{1-w_{ij}} \\
        = \prod_{i=1}^{S}\pi_i^{y_i}(1-\pi_i)^{t-y_i}.
    \end{split}
\end{equation}

The marginal distribution for the incidence-based frequency $Y_i$ for the $i$-th coverage element follows a binomial distribution characterized by $t$ and the detection probability $\pi_i$:

\begin{equation}
    P(Y_i=y_i) = \binom{t}{y_i}\pi_i^{y_i}(1-\pi_i)^{t-y_i}, \qquad i=1,2,\dots,S.
\end{equation}

Denote the incidence frequency counts by $(f_0, f_1, \dots, f_t)$, where $f_k$ is the number of elements covered in exactly $k$ sampling units in the data, $k=0,1,\dots,t$. Here, $f_1$ represents the number of \emph{singleton} elements (those that are covered in only one sampling unit), and $f_2$ represents the number of \emph{doubleton} elements (those that are covered in exactly two sampling units). The unobservable zero frequency count $f_0$ denotes the number of coverage elements that are not covered by any of the $t$ sampling units. Then, the number of covered elements in the current campaign is $S(t)=\sum_{i>0}f_i$, and $S(t)+f_0=S$.

%During a fuzzing campaign where we generate enormous amount of test inputs per unit time (at least thousands of inputs within seconds), it is not quite not practical to save the outcome for each input (i.e. the species $\mathcal{D}_i$ that our $X_n$ belongs to) in our data storage. Yet, it is a common practice to write such data at regular intervals. For instance, in our experimentation of 7-day fuzzing campaigns, we record incidence data once in every 15 minutes. Statistically, we can consider all the inputs generated between two consecutive recording instances as a sample of inputs and we indicate the detection/non-detection of species in this sample as a record. 
%

%Below we explain the probabilistic model for our fuzzing data. 
%
%\subsection{Probabilistic Model for Sampling-Unit-Based Data} \label{sec:probmodel}
%
%Assuming that the fuzzer's search space contains $S$ species, indexed by ${1,2,\dots,S}$. Without loss of generality, assume we have sampled $t$ sampling units, indexed by $1,2,\dots,t$, where each sampling unit represents a set of test inputs generated during a certain period of the ongoing fuzzing campaign. A species could be any choice of a program behaviour such as bugs or coverage elements like program paths, branches, or functions. It is assumed that these sampling units are randomly and independently sampled.
% ---- End Model
% --- Estimation
\section{Reachability Estimation in Fuzzing}
\label{sec:reachability}

%With the introduction of the STADS statistical framework and the growing adoption of fuzzing, understanding a fuzzer's detection capacity has become increasingly important. As a result, statistical techniques for estimating maximum reachability $\hat{S}$ have gained prominence \cite{stads,reachability_2023,roadmap}. Most estimators of $\hat{S}$ that conform to the probabilistic model in \autoref{sec:model} are introduced to fuzzing from the well-established field of ecological bio-statistics. However, challenges affecting the accuracy and precision of these estimates have been discussed both in the original STADS framework \cite{stads} and in more recent evaluations \cite{reachability_2023} of estimator performance. We discuss two critical challenges in more detail that we believe have a significant impact on reachability estimation and its evaluation in fuzzing.

\subsection{Adaptive bias}
A key statistical challenge in greybox fuzzing is \emph{adaptive bias}: as coverage-increasing inputs are added as new seeds, the distribution $\pi_i$ for $i: 1 \leq i \leq S$ evolves throughout the fuzzing campaign \cite{doctoral_2021}. A fundamental assumption of the Bernoulli Product model—common in statistics and machine learning—is that samples are independent and identically distributed (\emph{iid}), meaning the distribution of detection probabilities remains unchanged over time.  

This assumption holds in blackbox fuzzing campaigns where no new seeds are added to the seed corpus while the fuzzer is running. However, in greybox fuzzing, which is the predominant fuzzing technique today, the outcome of one sample influences subsequent samples, violating the independence assumption \cite{entropic,stads,doctoral_2021}. Consequently, applying existing statistical estimators to greybox fuzzing leads to \emph{adaptive bias}, where estimates systematically over- or under-estimate the true value in greybox campaigns.

Therefore, developing novel estimation strategies that correct adaptive bias in statistical estimation of key performance parameters is crucial for assessing greybox fuzzing progress and predicting its future trajectory. Recent advancements have introduced techniques that mitigate adaptive bias to estimate \emph{residual risk} \cite{residual2021} and to extrapolate \emph{coverage rate} \cite{coveragerate2024} in greybox fuzzing. However, state-of-the-art reachability estimators continue to be affected by adaptive bias.


\subsection{Challenge of Establishing Ground Truth for Evaluation}

As with other software testing techniques, fuzzing aims to progressively explore diverse program behaviors—such as increasing code coverage or triggering distinct faults—over time. However, a fundamental challenge in evaluating the effectiveness of a fuzzing campaign lies in the absence of an absolute ground truth. Specifically, we can never be certain that all reachable program behaviors have been exercised, regardless of how long the fuzzer runs \cite{dijkstra2002ewd}. There always exists a non-zero probability that previously undiscovered behaviors may emerge as new test inputs are generated \cite{residual2021}.

Despite this inherent uncertainty, the effectiveness of a fuzzing approach is often evaluated based on its maximum observed reachability within a fixed time or resource budget. For example, widely used benchmarks such as Fuzzbench \cite{metzman2021fuzzbench} and Magma \cite{hazimeh2020magma} assess fuzzers by measuring the coverage achieved after executing for a predetermined duration (e.g., 24 hours in the case of Fuzzbench). However, such evaluations can be misleading in scenarios where novel program behaviors are discovered beyond the allotted time window. This limitation underscores the need for more conclusive and robust quantitative techniques to assess fuzzing effectiveness—particularly methods grounded in data-driven estimation of maximum reachability. Still, establishing the ground truth remains a cornerstone for reliably evaluating such quantitative reachability measures.

\subsection{Existing Methods in Approximating Ground-Truth for Maximum Reachability}

Even though achieving a 100\% guarantee of exhausting all possible program behaviors is inherently unattainable, the longer a fuzzing campaign proceeds without encountering new behaviors, the lower the probability of discovering additional ones becomes. This diminishing discovery rate, in turn, increases our confidence in treating the current state as an empirical approximation of the ground truth for maximum reachability. However, recent empirical evidence based on extensive 7-day AFL++ \cite{aflpp} fuzzing campaigns across 32 real-world C programs demonstrates that reaching a true asymptote in coverage is often elusive in practice \cite{reachability_2023}. While coverage curves may appear to saturate when plotted on a linear scale, this effect is far less discernible on a logarithmic scale—suggesting that new behaviors continue to emerge at a slow but non-negligible rate. This observation further complicates the task of defining and approximating ground truth in fuzzing evaluation. Moreover, the same study highlights the limitations of using static analysis for estimating maximum reachable coverage, revealing both under- and over-approximation in practice. 

To address this challenge, the current fuzzing literature proposes state-of-the-art strategies to approximate ground-truth reachability, particularly for evaluating statistical estimators of maximum reachability:

\begin{enumerate}
    \item \textbf{Using small-scale programs as subjects:} When the subject program is relatively small and contains only a limited number of distinct reachable behaviors, fuzzers are more likely to achieve saturation within a feasible time frame. This makes such programs suitable proxies for maximum reachability estimator evaluations with ground-truth.
    
    \item \textbf{Bootstrapping from campaign data:} After executing a fuzzer for a sufficiently long period—ideally until the coverage or bug discovery curve exhibits signs of plateauing—bootstrap resampling can be applied to the observed data. The key intuition is that the relationship between the empirical distribution of discovered behaviors and the underlying theoretical population mirrors the relationship between bootstrap samples and the empirical distribution. This method enables statistical approximation of ground-truth reachability without requiring absolute exhaustiveness.
\end{enumerate}

\subsection{Drawbacks of Existing Ground-Truth Approaches}

Despite efforts to approximate ground truth in fuzzing evaluation, existing methods, particularly those relying on small-scale programs, have significant limitations. While small programs enable faster saturation of program behaviors, they fail to reflect the complexity and diversity of real-world systems, which often exhibit a highly skewed distribution of behaviors—similar to ecological environments with a few abundant behaviors and many rare ones. In contrast, smaller programs tend to have a less skewed distribution of species, leading to an overestimation of fuzzing estimators’ accuracy. This discrepancy results in misleading conclusions when performance estimators derived from small-scale programs are applied to larger, more complex systems, where the species distribution is less predictable. Consequently, the primary drawback of using small-scale programs for ground-truth approximation is the risk of overstating the effectiveness of fuzzing approaches when applied to real-world software.

In the case of bootstrapping, the core assumption—that the empirical distribution obtained from a fuzzing campaign reasonably approximates the true underlying distribution of all possible program behaviors—fails to satisfy when the campaign has not yet thoroughly explored critical regions of the codebase or when input generation is inherently biased. This assumption further relies on the expectation that the future trajectory of behavior discovery will not significantly deviate from its current trend, implying that species accumulation will smoothly approach saturation. However, without additional quantitative evidence to support the stability of this trajectory—such as statistical indicators of diminishing returns or convergence metrics—such assumptions remain speculative. In the absence of such validation, bootstrapped estimates may result in substantial over- or under-estimations of maximum reachability, ultimately undermining the reliability of fuzzing effectiveness evaluations.  

These limitations highlight the need for more robust, scalable, and accurate methods of defining and approximating ground truth in fuzzing evaluation, particularly as programs grow in size and complexity.

% --- End Estimation
\input{methodology.tex}
\input{setup.tex}
\input{related.tex}


\section{Conclusion}
\label{sec:conclusion}
In this report, we propose to assess the reliability of species estimators when
they are used for estimating reachable coverage. We propose to do that both by
providing a synthetic benchmark with labelled ground truth, and also by
using a separate means of checking the reliability of estimators by variying
the sampling units used.

%\section{Authors and Affiliations}


\bibliographystyle{IEEEtran}
\bibliography{references.bib}


%\section{Acknowledgments}

%% If your work has an appendix, this is the place to put it.
%\appendix

\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
